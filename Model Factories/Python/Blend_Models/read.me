<!-- USAGE EXAMPLES -->
## Usage

To show how to tune multiple DataRobot text models and then use a blender to combine the prediction, we decided to use the fake_job_postings.csv from Aegan, which can also be found in Kaggle.  The project take the fake_job_postings.csv clean the dataset and engineer new features.
The dataset is in ./Data
The directory Data has the prepared dataset, and the performance of all the models built in DataRobot.
While the preparation of the dataset is particular to fake_job_postings, the rest of the code is independent of the original dataset.  
###Assumption:
There are at least 2 text features
###Functions and their purpose
####Prepare Dataset 
get_min_max_salary (text)
cleaned_location(text)
PrepareDataSet()
###Start DataRobot Project, and extract text features
In some cases, you may think that a feature is a text feature but DataRobot will set its type to a categorical feature

start_project_with_settings(fake_jobs_df
get_text_features(project)
get_1_model_performance(model_p,text_feature,num_modified)
models_performance_for_text(text_feature,project):
get_best_models_before_text(project):
prepare_model_for_tuning(model_id,project)
set_parameters(param_2_set,tune)
run_auto_tuned_with_different_settings(models_df,project)
get_performance_of_tuned_models(models_tuned,project)
create_new_featurelist(project)
run_best_model_with_new_featurelist(project,bestModels,sample_size_train,featurelist)
get_All_model_performance(model_p)
models_performance_for_All(project)
extract_best_models(result_df)
rerun_text_models_with_new_param(project)
create_df_with_performance_of_all_models(project,result_df,bestModels)
