{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datarobot as dr\n",
    "from datarobot import Project, Deployment\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "import dateutil.parser\n",
    "import os\n",
    "import re \n",
    "from importlib import reload\n",
    "import random\n",
    "import math\n",
    "import numpy.ma as ma\n",
    "\n",
    "import timesynth as ts\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# Set Pandas configuration to show all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dr.Client(config_path='../drconfig.yaml');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"\"\" Enter Code \"\"\")\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MONTHS = '' \n",
    "\n",
    "def months(df):\n",
    "    global MONTHS\n",
    "    MIN_DATE = df['Date'].min()\n",
    "    MAX_DATE = df['Date'].max()\n",
    "    MONTHS = str(int((MAX_DATE - MIN_DATE).days / 30))\n",
    "     \n",
    "    print('Min Date: ', MIN_DATE)\n",
    "    print('Max Date: ', MAX_DATE)\n",
    "    print('Months:   ', MONTHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create TS Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATE      = \"\"\" Enter Code \"\"\"\n",
    "TARGET    = \"\"\" Enter Code \"\"\"\n",
    "\n",
    "PROJECT_NAME = 'Lab_3'\n",
    "\n",
    "VERSION = '1'\n",
    "MODE    = 'Q'\n",
    "    \n",
    "FDWS = \"\"\" Enter Code \"\"\"\n",
    "\n",
    "FDS  = \"\"\" Enter Code \"\"\" \n",
    "\n",
    "BASE   = 'L3_2_V:'\n",
    "\n",
    "PREFIX = BASE + VERSION + '_Mnths:' + MONTHS + '_Mode:' + MODE\n",
    "DATASET_FILENAME = 'Months_' + MONTHS\n",
    "MAX_WAIT = 14400\n",
    "READ_TIMEOUT = 14400\n",
    "\n",
    "\n",
    "HOLDOUT_START_DATE  = None \n",
    "VALIDATION_DURATION = None \n",
    "HOLDOUT_DURATION    = None \n",
    "NUMBER_BACKTESTS    = None\n",
    "GAP_DURATION        = None \n",
    "\n",
    "FEATURE_SETTINGS = []\n",
    "\n",
    "CAL_ID = None\n",
    "\n",
    "print(FEATURE_SETTINGS)\n",
    "print(CAL_ID)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to create projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dr_project(df, project_name, fw_start=None, fw_end=None, fdw_start=None, fdw_end=None, dataset_filename=DATASET_FILENAME):\n",
    "    ###############################\n",
    "    # Create Datetime Specification\n",
    "    ###############################\n",
    "\n",
    "    time_partition = dr.DatetimePartitioningSpecification(\n",
    "        datetime_partition_column = DATE,\n",
    "        forecast_window_start     = fw_start, \n",
    "        forecast_window_end       = fw_end,\n",
    "        feature_derivation_window_start = fdw_start,\n",
    "        feature_derivation_window_end   = fdw_end,\n",
    "        holdout_start_date        = HOLDOUT_START_DATE ,\n",
    "        validation_duration       = VALIDATION_DURATION,  \n",
    "        holdout_duration          = HOLDOUT_DURATION,\n",
    "        number_of_backtests       = NUMBER_BACKTESTS, \n",
    "        feature_settings          = FEATURE_SETTINGS,\n",
    "        use_time_series           = True\n",
    "      )\n",
    "     \n",
    "\n",
    "    ################\n",
    "    # Create Project\n",
    "    ################\n",
    "    project = dr.Project.create(\n",
    "        project_name = project_name, \n",
    "        sourcedata   = df, \n",
    "        max_wait     = MAX_WAIT, \n",
    "        read_timeout = READ_TIMEOUT,\n",
    "        dataset_filename = DATASET_FILENAME\n",
    "    )\n",
    "    print(\"Post-Project MB: \", (df.memory_usage(index=True).sum()/1024/1024).round(2))\n",
    "    print(\"Post-Project Records: {:,}\".format(len(df)))\n",
    "    print(f'Project {project_name} Created...')\n",
    "\n",
    "    #################\n",
    "    # Start Autopilot\n",
    "    #################\n",
    "    project.set_target(\n",
    "        target = TARGET,   \n",
    "        metric = None,      \n",
    "        mode   = dr.AUTOPILOT_MODE.QUICK , # dr.AUTOPILOT_MODE.FULL_AUTO,\n",
    "        #advanced_options = opts,\n",
    "        worker_count = -1,\n",
    "        partitioning_method = time_partition,\n",
    "        max_wait = MAX_WAIT\n",
    "    )\n",
    "    return project\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects = []  # Keep List of all project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to iterate through various FDW's and FD's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_factory(df, FDWS, FDS):\n",
    "    PREFIX = BASE + str(VERSION) + '_Mnths:' + MONTHS + '_Mode:' + MODE\n",
    "    DATASET_FILENAME = 'Months_' + MONTHS\n",
    "    \n",
    "    for fdw in FDWS:\n",
    "        for fd in FDS:\n",
    "            fd_start  = fd[0] \n",
    "            fd_end    = fd[1]\n",
    "            fdw_start = fdw[0]\n",
    "            fdw_end   = fdw[1]\n",
    "\n",
    "            # Name project\n",
    "            project_name = f\"{PREFIX}_FDW:{fdw_start}-{fdw_end}_FD:{fd_start}-{fd_end}\"  \n",
    "            print(project_name)\n",
    "\n",
    "            data = df.copy() \n",
    "\n",
    "            # Create project\n",
    "            project = create_dr_project(data, project_name, \n",
    "                                        fw_start=fd_start, fw_end=fd_end, \n",
    "                                        fdw_start=fdw_start, fdw_end=fdw_end,\n",
    "                                        dataset_filename=DATASET_FILENAME)\n",
    "\n",
    "            projects.append(project) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_factory(df, FDWS, FDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Truncated dataset to 24 Months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_24 = df.loc[df['Date'] >= pd.to_datetime(\"\"\" Enter Code \"\"\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months(df_24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_factory(df_24, FDWS, FDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Truncated dataset to 18 Months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_18 = df.loc[df['Date'] >= pd.to_datetime(\"\"\" Enter Code \"\"\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months(df_18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_factory(df_18, FDWS, FDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Project Names in a List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects = dr.Project.list(search_params={'project_name': BASE}) \n",
    "projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Project Names and PIDs in a List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = []\n",
    "\n",
    "for p in projects:\n",
    "    r = ((p, p.id))\n",
    "    lst.append(r)\n",
    "lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unlock Holdouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in lst:\n",
    "    project = Project.get(i[1])\n",
    "    project.unlock_holdout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Backtests for Blenders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in lst:\n",
    "    project = Project.get(i[1])\n",
    "    lb = project.get_datetime_models()\n",
    "    for model in lb:\n",
    "        \n",
    "        if 'Blender' in model.model_type:\n",
    "            try:\n",
    "                print(project.project_name, model)  \n",
    "                dr.DatetimeModel.score_backtests(model) \n",
    "                print(f'Computing backtests for model {model.id} in Project {project.project_name}')\n",
    "            except dr.errors.ClientError:\n",
    "                pass\n",
    "            print(f'All available backtests have been submitted for scoring for project {project.project_name}')\n",
    "            print(' ')\n",
    "        else:\n",
    "            None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute All Backtests for Top Models in Backtest 1 and Holdout groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPTIMIZE_GROUP = ['validation', 'holdout']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_METRIC = project.metric\n",
    "METRICS = list(set([PROJECT_METRIC, 'MASE', 'RMSE']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in lst :\n",
    "    for met in METRICS:\n",
    "        for o in OPTIMIZE_GROUP:\n",
    "            project = Project.get(p[1])\n",
    "            lb = project.get_datetime_models()\n",
    "\n",
    "            best_models = sorted(\n",
    "                                [model for model in lb if model.metrics[met][o]],  \n",
    "                                key=lambda m: m.metrics[met][o],\n",
    "                                )[0:3]\n",
    "            \n",
    "            for mod in best_models:\n",
    "\n",
    "                if mod.metrics[met][\"backtesting\"] == None:\n",
    "                    try:\n",
    "                        print(project.project_name, mod)  \n",
    "                        dr.DatetimeModel.score_backtests(mod) \n",
    "                        print(f'Computing backtests for model {mod.model_type} in Project {project.project_name}')\n",
    "                    except dr.errors.ClientError:\n",
    "                        pass\n",
    "                    print(f'All available backtests have been submitted for scoring for project {project.project_name}')\n",
    "                    print(' ')\n",
    "                else:\n",
    "                    print(project.project_name)\n",
    "                    print(f'{mod.model_type} All Backtests Already Computed')\n",
    "                    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Project and Model Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPTIMIZATION_PERIOD = 'backtesting'  # BackTest 1: validation  All Backtest: backtesting  Holdout: holdout "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "scores = pd.DataFrame()\n",
    "\n",
    "for p in lst:\n",
    "    project = Project.get(p[1])\n",
    "    lb = project.get_datetime_models()\n",
    "    best_model = sorted(\n",
    "                        [model for model in lb if model.metrics[project.metric][OPTIMIZATION_PERIOD]],  \n",
    "                        key=lambda m: m.metrics[project.metric][OPTIMIZATION_PERIOD],\n",
    "                        )[0]\n",
    "\n",
    "    backtest_scores = pd.DataFrame(\n",
    "        [\n",
    "            {\n",
    "                'Project_Name': project.project_name,\n",
    "                'Project_ID': project.id,\n",
    "                'Model_ID': best_model.id,\n",
    "                'Model_Type': best_model.model_type,\n",
    "                'Featurelist': best_model.featurelist_name,\n",
    "                'Optimization_Metric': project.metric,\n",
    "                'Scores': best_model.metrics,\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    scores = scores.append(backtest_scores, sort=False).reset_index(drop=True)  \n",
    "\n",
    "\n",
    "print(f'Scores for all {len(projects)} projects have been computed')\n",
    "print('')\n",
    "\n",
    "scores = scores.join(json_normalize(scores[\"Scores\"].tolist())).drop(labels=['Scores'], axis=1) \n",
    "\n",
    "# Drop Empty Columns\n",
    "scores = scores[scores.columns.drop(list(scores.filter(regex='crossValidation$')))]\n",
    "\n",
    "# Rename Columns\n",
    "scores.columns = scores.columns.str.replace(\".backtesting\", \"_All_BT\")\n",
    "scores.columns = scores.columns.str.replace(\".holdout\", \"_Holdout\")\n",
    "scores.columns = scores.columns.str.replace(\".validation\", \"_BT_1\")\n",
    "scores.columns = scores.columns.str.replace(' ', '_')\n",
    "\n",
    "scores = scores[scores.columns.drop(list(scores.filter(regex='_All_BTScores$')))]\n",
    "\n",
    "scores.head(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create lists of columns to view for easy reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = scores.filter(regex='MASE|RMSE').columns.to_list()\n",
    "PROJECT = ['Project_Name', 'Project_ID', 'Model_ID', 'Model_Type', 'Featurelist']\n",
    "COLS = PROJECT + METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores[COLS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort dataset by a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores['FDW_Start'] = scores['Project_Name'].str.extract(r'FDW:(-\\d{1,2})')\n",
    "scores['FDW_End']   = scores['Project_Name'].str.extract(r'FDW:-\\d{1,2}-(\\d{1,2})_')\n",
    "scores['FD_Start']  = scores['Project_Name'].str.extract(r'FD:(\\d{1,2})')\n",
    "scores['FD_End']    = scores['Project_Name'].str.extract(r'FD:\\d{1,2}-(\\d{1,2})')\n",
    "scores['Months']    = scores['Project_Name'].str.extract(r'_Mnths:(\\d{1,2})_')\n",
    "\n",
    "scores.rename(columns={'All_Backtests_Poisson Deviance':'All_Backtests_Poisson_Deviance', \n",
    "                       'Backtest_1_Poisson Deviance':'Backtest_1_Poisson_Deviance',\n",
    "                       'Holdout_Poisson Deviance':'Holdout_Poisson_Deviance',\n",
    "                       'Holdout_Tweedie Deviance':'Holdout_Tweedie_Deviance',\n",
    "                       'All_Backtests_Tweedie Deviance':'All_Backtests_Tweedie_Deviance',\n",
    "                       'Backtest_1_Tweedie Deviance':'Backtest_1_Tweedie_Deviance',\n",
    "                       'Holdout_Tweedie Deviance':'Holdout_Tweedie_Deviance'}, inplace=True)\n",
    "\n",
    "\n",
    "META = ['FDW_Start', 'FDW_End', 'FD_Start', 'FD_End', 'Months']\n",
    "MORE = PROJECT + META + METRICS \n",
    "  \n",
    "scores[MORE].sort_values(by=[\"\"\" Enter Code \"\"\"], ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrmse = scores.loc[scores[\"\"\" Enter Code \"\"\"].notnull()]\n",
    "\n",
    "# Take the Single Best model\n",
    "# hrmse_best = hrmse.loc[hrmse.RMSE_All_BT.idxmin()]\n",
    "\n",
    "# Take the Best model by Project Name\n",
    "hrmse_best = hrmse.loc[hrmse.groupby('Project_Name').MASE_All_BT.idxmin()]\n",
    "\n",
    "best_models = pd.DataFrame(hrmse_best) \n",
    "best_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
