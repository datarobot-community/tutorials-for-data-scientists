{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datarobot as dr\n",
    "from datarobot import Project, Deployment\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "import dateutil.parser\n",
    "import os\n",
    "import re \n",
    "from importlib import reload\n",
    "import random\n",
    "import math\n",
    "import numpy.ma as ma\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# Set Pandas configuration to show all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dr.Client(config_path='../drconfig.yaml');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"\"\" Enter Code \"\"\")\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(x='Date', y='Rides', figsize=(20, 8));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Months (Length of training data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MONTHS = '' \n",
    "\n",
    "def months(df):\n",
    "    global MONTHS\n",
    "    MIN_DATE = df['Date'].min()\n",
    "    MAX_DATE = df['Date'].max()\n",
    "    MONTHS = str(int((MAX_DATE - MIN_DATE).days / 30))\n",
    "     \n",
    "    print('Min Date: ', MIN_DATE)\n",
    "    print('Max Date: ', MAX_DATE)\n",
    "    print('Months:   ', MONTHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create TS Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATE      = \"\"\" Enter Code \"\"\"\n",
    "TARGET    = \"\"\" Enter Code \"\"\"\n",
    "\n",
    "PROJECT_NAME = 'Lab_3'\n",
    "\n",
    "VERSION = '1'\n",
    "MODE    = 'Q'\n",
    "    \n",
    "FDWS = \"\"\" Enter Code \"\"\"\n",
    "\n",
    "FDS  = \"\"\" Enter Code \"\"\" \n",
    "\n",
    "\n",
    "BASE   = 'L3_1_V:'\n",
    "\n",
    "PREFIX = BASE + VERSION + '_Mnths:' + MONTHS + '_Mode:' + MODE\n",
    "DATASET_FILENAME = 'Months_' + MONTHS\n",
    "MAX_WAIT = 14400\n",
    "READ_TIMEOUT = 14400\n",
    "\n",
    "HOLDOUT_START_DATE  = None\n",
    "VALIDATION_DURATION = None\n",
    "HOLDOUT_DURATION    = None\n",
    "NUMBER_BACKTESTS    = 4 # Keep Backtests at 4\n",
    "GAP_DURATION        = None \n",
    "\n",
    "FEATURE_SETTINGS = []\n",
    "\n",
    "CAL_ID = None\n",
    "\n",
    "print(FEATURE_SETTINGS)\n",
    "print(CAL_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create function to create projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dr_project(df, project_name, fw_start=None, fw_end=None, fdw_start=None, fdw_end=None, dataset_filename=DATASET_FILENAME):\n",
    "    \n",
    "    ###############################\n",
    "    # Create Datetime Specification\n",
    "    ###############################\n",
    "    # SERIES_COL = [SERIES]\n",
    "    time_partition = dr.DatetimePartitioningSpecification(\n",
    "        datetime_partition_column = DATE,\n",
    "        forecast_window_start     = fw_start, \n",
    "        forecast_window_end       = fw_end,\n",
    "        feature_derivation_window_start = fdw_start,\n",
    "        feature_derivation_window_end   = fdw_end,\n",
    "        holdout_start_date        = HOLDOUT_START_DATE ,\n",
    "        validation_duration       = VALIDATION_DURATION,  \n",
    "        holdout_duration          = HOLDOUT_DURATION,\n",
    "        gap_duration              = GAP_DURATION,\n",
    "        number_of_backtests       = NUMBER_BACKTESTS, \n",
    "        feature_settings          = FEATURE_SETTINGS,\n",
    "        use_time_series           = True,\n",
    "        calendar_id               = CAL_ID\n",
    "      )\n",
    "     \n",
    "\n",
    "    ################\n",
    "    # Create Project\n",
    "    ################\n",
    "    project = dr.Project.create(\n",
    "        project_name = project_name, \n",
    "        sourcedata   = df, \n",
    "        max_wait     = MAX_WAIT, \n",
    "        read_timeout = READ_TIMEOUT,\n",
    "        dataset_filename = DATASET_FILENAME\n",
    "    )\n",
    "    print(\"Post-Project MB: \", (df.memory_usage(index=True).sum()/1024/1024).round(2))\n",
    "    print(\"Post-Project Records: {:,}\".format(len(df)))\n",
    "    print(f'Project {project_name} Created...')\n",
    "\n",
    "    #################\n",
    "    # Start Autopilot\n",
    "    #################\n",
    "    project.set_target(\n",
    "        target = TARGET,   \n",
    "        metric = None,      \n",
    "        mode   = dr.AUTOPILOT_MODE.QUICK , # dr.AUTOPILOT_MODE.FULL_AUTO,\n",
    "        #advanced_options = opts,\n",
    "        worker_count = -1,\n",
    "        partitioning_method = time_partition,\n",
    "        max_wait = MAX_WAIT\n",
    "    )\n",
    "    return project\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Factory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to loop through the various FDWs & FDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects = []  # Keep List of all projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_factory(df, FDWS, FDS):\n",
    "    PREFIX = BASE + str(VERSION) + '_Mnths:' + MONTHS + '_Mode:' + MODE\n",
    "    DATASET_FILENAME = 'Months_' + MONTHS\n",
    "    \n",
    "    for fdw in FDWS:\n",
    "        for fd in FDS:\n",
    "            fd_start  = fd[0] \n",
    "            fd_end    = fd[1]\n",
    "            fdw_start = fdw[0]\n",
    "            fdw_end   = fdw[1]\n",
    "\n",
    "            # Name project\n",
    "            project_name = f\"{PREFIX}_FDW:{fdw_start}-{fdw_end}_FD:{fd_start}-{fd_end}\"  \n",
    "            print(project_name)\n",
    "\n",
    "            data = df.copy() \n",
    "\n",
    "            # Create project\n",
    "            project = create_dr_project(data, project_name, \n",
    "                                        fw_start=fd_start, fw_end=fd_end, \n",
    "                                        fdw_start=fdw_start, fdw_end=fdw_end,\n",
    "                                        dataset_filename=DATASET_FILENAME)\n",
    "\n",
    "            projects.append(project) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_factory(df, FDWS, FDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Holiday Optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update Version\n",
    "VERSION = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOLDOUT_START_DATE  = \"\"\" Enter Code \"\"\" \n",
    "VALIDATION_DURATION = dr.helpers.partitioning_methods.construct_duration_string(\"\"\" Enter Code \"\"\")\n",
    "HOLDOUT_DURATION    = dr.helpers.partitioning_methods.construct_duration_string(\"\"\" Enter Code \"\"\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_factory(df, FDWS, FDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pull Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Project Names in a List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects = dr.Project.list(search_params={'project_name': BASE}) \n",
    "projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Project Names and PIDs in a List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = []\n",
    "\n",
    "for p in projects:\n",
    "    r = ((p, p.id))\n",
    "    lst.append(r)\n",
    "lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unlock Holdouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in lst:\n",
    "    project = Project.get(i[1])\n",
    "    project.unlock_holdout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Backtests for Blenders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in lst:\n",
    "    project = Project.get(i[1])\n",
    "    lb = project.get_datetime_models()\n",
    "    for model in lb:\n",
    "        \n",
    "        if 'Blender' in model.model_type:\n",
    "            try:\n",
    "                print(project.project_name, model)  \n",
    "                dr.DatetimeModel.score_backtests(model) \n",
    "                print(f'Computing backtests for model {model.id} in Project {project.project_name}')\n",
    "            except dr.errors.ClientError:\n",
    "                pass\n",
    "            print(f'All available backtests have been submitted for scoring for project {project.project_name}')\n",
    "            print(' ')\n",
    "        else:\n",
    "            None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute All Backtests for Top Models in Backtest 1 and Holdout groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPTIMIZE_GROUP = ['validation', 'holdout']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_METRIC = project.metric\n",
    "METRICS = list(set([PROJECT_METRIC, 'MASE', 'RMSE']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in lst :\n",
    "    for met in METRICS:\n",
    "        for o in OPTIMIZE_GROUP:\n",
    "            project = Project.get(p[1])\n",
    "            lb = project.get_datetime_models()\n",
    "\n",
    "            best_models = sorted(\n",
    "                                [model for model in lb if model.metrics[met][o]],  \n",
    "                                key=lambda m: m.metrics[met][o],\n",
    "                                )[0:3]\n",
    "            \n",
    "            for mod in best_models:\n",
    "\n",
    "                if mod.metrics[met][\"backtesting\"] == None:\n",
    "                    try:\n",
    "                        print(project.project_name, mod)  \n",
    "                        dr.DatetimeModel.score_backtests(mod) \n",
    "                        print(f'Computing backtests for model {mod.model_type} in Project {project.project_name}')\n",
    "                    except dr.errors.ClientError:\n",
    "                        pass\n",
    "                    print(f'All available backtests have been submitted for scoring for project {project.project_name}')\n",
    "                    print(' ')\n",
    "                else:\n",
    "                    print(project.project_name)\n",
    "                    print(f'{mod.model_type} All Backtests Already Computed')\n",
    "                    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Project and Model Scores in a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPTIMIZATION_PERIOD = 'validation'  # BackTest 1: validation  All Backtest: backtesting  Holdout: holdout "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "scores = pd.DataFrame()\n",
    "\n",
    "\n",
    "for p in lst:\n",
    "    project = Project.get(p[1])\n",
    "    lb = project.get_datetime_models()\n",
    "    best_model = sorted(\n",
    "                        [model for model in lb if model.metrics[project.metric][OPTIMIZATION_PERIOD]],  \n",
    "                        key=lambda m: m.metrics[project.metric][OPTIMIZATION_PERIOD],\n",
    "                        )[0]\n",
    "\n",
    "    backtest_scores = pd.DataFrame(\n",
    "        [\n",
    "            {\n",
    "                'Project_Name': project.project_name,\n",
    "                'Project_ID': project.id,\n",
    "                'Model_ID': best_model.id,\n",
    "                'Model_Type': best_model.model_type,\n",
    "                'Featurelist': best_model.featurelist_name,\n",
    "                'Optimization_Metric': project.metric,\n",
    "                'Scores': best_model.metrics,\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    scores = scores.append(backtest_scores, sort=False).reset_index(drop=True)  \n",
    "\n",
    "\n",
    "print(f'Scores for all {len(projects)} projects have been computed')\n",
    "print('')\n",
    "\n",
    "scores = scores.join(json_normalize(scores[\"Scores\"].tolist())).drop(labels=['Scores'], axis=1) \n",
    "\n",
    "# Drop Empty Columns\n",
    "scores = scores[scores.columns.drop(list(scores.filter(regex='crossValidation$')))]\n",
    "\n",
    "# Rename Columns\n",
    "scores.columns = scores.columns.str.replace(\".backtesting\", \"_All_BT\")\n",
    "scores.columns = scores.columns.str.replace(\".holdout\", \"_Holdout\")\n",
    "scores.columns = scores.columns.str.replace(\".validation\", \"_BT_1\")\n",
    "scores.columns = scores.columns.str.replace(' ', '_')\n",
    "\n",
    "scores = scores[scores.columns.drop(list(scores.filter(regex='_All_BTScores$')))]\n",
    "\n",
    "scores.head(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select subset of columns into varibles for easy reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = scores.filter(regex='MASE|RMSE').columns.to_list()\n",
    "PROJECT = ['Project_Name', 'Project_ID', 'Model_ID', 'Model_Type', 'Featurelist']\n",
    "COLS = PROJECT + METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores[COLS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores['FDW_Start'] = scores['Project_Name'].str.extract(r'FDW:(-\\d{1,2})')\n",
    "scores['FDW_End']   = scores['Project_Name'].str.extract(r'FDW:-\\d{1,2}-(\\d{1,2})_')\n",
    "scores['FD_Start']  = scores['Project_Name'].str.extract(r'FD:(\\d{1,2})')\n",
    "scores['FD_End']    = scores['Project_Name'].str.extract(r'FD:\\d{1,2}-(\\d{1,2})')\n",
    "scores['Months']    = scores['Project_Name'].str.extract(r'_Mnths:(\\d{1,2})_')\n",
    "\n",
    "scores.rename(columns={'All_Backtests_Poisson Deviance':'All_Backtests_Poisson_Deviance', \n",
    "                       'Backtest_1_Poisson Deviance':'Backtest_1_Poisson_Deviance',\n",
    "                       'Holdout_Poisson Deviance':'Holdout_Poisson_Deviance',\n",
    "                       'Holdout_Tweedie Deviance':'Holdout_Tweedie_Deviance',\n",
    "                       'All_Backtests_Tweedie Deviance':'All_Backtests_Tweedie_Deviance',\n",
    "                       'Backtest_1_Tweedie Deviance':'Backtest_1_Tweedie_Deviance',\n",
    "                       'Holdout_Tweedie Deviance':'Holdout_Tweedie_Deviance'}, inplace=True)\n",
    "\n",
    "\n",
    "META = ['FDW_Start', 'FDW_End', 'FD_Start', 'FD_End', 'Months']\n",
    "MORE = PROJECT + META + METRICS \n",
    "\n",
    "\n",
    "# Sort by the correct partition\n",
    "scores[MORE].sort_values(by=[\"\"\" Enter Code \"\"\"], ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrmse = scores.loc[scores[\"\"\" Enter Code \"\"\"].notnull()]\n",
    "\n",
    "# Take the Single Best model\n",
    "# hrmse_best = pd.DataFrame(hrmse.loc[hrmse.MASE_All_BT.idxmin()]).transpose()\n",
    "\n",
    "# Take the Best model by Project Name\n",
    "hrmse_best = hrmse.loc[hrmse.groupby('Project_Name').RMSE_BT_1.idxmin()]\n",
    "\n",
    "best_models = pd.DataFrame(hrmse_best) \n",
    "best_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Holiday Optimized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the record with the project that has the holdout set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RECORD = \"\"\" Enter Code \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PID = best_models['Project_ID'].values[RECORD]\n",
    "MID = best_models['Model_ID'].values[RECORD]\n",
    "\n",
    "project = dr.Project.get(PID)\n",
    "model   = dr.Model.get(PID, MID)\n",
    "print(project)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain on Frozen parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enter the start and end dates to re-train the model on with frozen parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_START_DATE    = pd.to_datetime(\"\"\" Enter Code \"\"\")\n",
    "TRAINING_END_DATE      = pd.to_datetime(\"\"\" Enter Code \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = model.request_frozen_datetime_model( training_row_count     = None, \n",
    "                                           training_duration      = None,\n",
    "                                           training_start_date    = TRAINING_START_DATE,\n",
    "                                           training_end_date      = TRAINING_END_DATE,\n",
    "                                           time_window_sample_pct = None  \n",
    "                                          )\n",
    "\n",
    "retrained_model = job.get_result_when_complete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PID = best_models['Project_ID'].values[RECORD]\n",
    "MID = best_models['Model_ID'].values[RECORD]\n",
    "\n",
    "project = dr.Project.get(PID)\n",
    "print(project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model   = retrained_model \n",
    "\n",
    "dataset = project.upload_dataset(\"\"\" Enter Code \"\"\")\n",
    "\n",
    "pred_job = model.request_predictions(dataset_id = dataset.id)\n",
    "\n",
    "H_preds = pred_job.get_result_when_complete()\n",
    "\n",
    "H_preds['timestamp']      = pd.to_datetime(H_preds['timestamp'], utc=True)\n",
    "H_preds['forecast_point'] = pd.to_datetime(H_preds['forecast_point'], utc=True)\n",
    "\n",
    "H_preds.rename(columns={'timestamp':'Date', 'prediction':'Holiday_Pred'}, inplace=True)\n",
    "\n",
    "H_preds.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summer Optimized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the record with the project that has the default partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RECORD = \"\"\" Enter Code \"\"\"\n",
    "\n",
    "# Verify correct project\n",
    "PID = best_models['Project_ID'].values[RECORD]\n",
    "MID = best_models['Model_ID'].values[RECORD]\n",
    "\n",
    "project = dr.Project.get(PID)\n",
    "model   = dr.Model.get(PID, MID)\n",
    "print(project, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PID = best_models['Project_ID'].values[RECORD]\n",
    "MID = best_models['Model_ID'].values[RECORD]\n",
    "\n",
    "project = dr.Project.get(PID)\n",
    "print(project.project_name)\n",
    "print(\" \")\n",
    "\n",
    "model   = dr.Model.get(PID, MID)\n",
    "\n",
    "dataset = project.upload_dataset(\"\"\" Enter Code \"\"\")\n",
    "\n",
    "pred_job = model.request_predictions(dataset_id = dataset.id)\n",
    "\n",
    "S_preds = pred_job.get_result_when_complete()\n",
    "\n",
    "S_preds['timestamp']      = pd.to_datetime(S_preds['timestamp'], utc=True)          \n",
    "S_preds['forecast_point'] = pd.to_datetime(S_preds['forecast_point'], utc=True)\n",
    "\n",
    "S_preds.rename(columns={'timestamp':'Date', 'prediction':'Summer_Pred'}, inplace=True)\n",
    "\n",
    "S_preds.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the CTA_actuals dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actuals = pd.read_csv(\"\"\" Enter Code \"\"\")\n",
    "\n",
    "actuals['Date'] =  pd.to_datetime(actuals['Date'], utc=True)      \n",
    "actuals.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Summer vs Holiday Model Forecasts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge the Actuals, Summer, and Fall datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = \"\"\" Enter Code \"\"\"\n",
    "results['Summer_Pred']  = results['Summer_Pred'].astype(int)\n",
    "results['Holiday_Pred'] = results['Holiday_Pred'].astype(int)\n",
    "results.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.plot(x='Date', figsize=(22, 8));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actuals = results.Rides\n",
    "summer  = results.\"\"\" Enter Code \"\"\"\n",
    "holiday = results.\"\"\" Enter Code \"\"\"\n",
    "\n",
    "Summer_rmse  = mean_squared_error(\"\"\" Enter Code \"\"\", \"\"\" Enter Code \"\"\", squared=\"\"\" Enter Code \"\"\")\n",
    "Holiday_rmse = mean_squared_error(\"\"\" Enter Code \"\"\", \"\"\" Enter Code \"\"\", squared=\"\"\" Enter Code \"\"\")\n",
    "\n",
    "print('Summer RMSE:  {:,.2f}'.format(Summer_rmse))\n",
    "print('Holiday RMSE: {:,.2f}'.format(Holiday_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
