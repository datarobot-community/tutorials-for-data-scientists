1\. R API Training - Feature Selection Curves \[Solution\]
================
Thodoris Petropoulos and Chester Ismay<br>Contributors: Rajiv Shah

Hey again\! This is the first in a series of exercises to complete in
order to finish your `R API Training for DataRobot` course\! This
exercise will help you learn how to manipulate `DataRobot Models` and
`Feature Lists`.

Here are the actual sections of the notebook alongside time to complete:

1.  Connect to DataRobot. \[3min\]<br>
2.  Create a Project. \[15min\]<br>
3.  Create Custom Feature Lists. \[15min\]<br>
4.  Identify Specific Models. \[20min\]<br>
5.  Retrain Models on the custom feature lists. \[10min\] <br>
6.  Plot performance based on the different feature lists. \[30min\]
7.  Bonus Question

Each section will have specific instructions so do not worry if things
are still blurry\!

As always, consult:

  - [API Documentation via CRAN
    Vignettes](https://CRAN.R-project.org/package=datarobot)
  - [Samples](https://github.com/datarobot-community/examples-for-data-scientists)
  - [Tutorials](https://github.com/datarobot-community/tutorials-for-data-scientists)

The last two links should provide you with the snippets you need to
complete most of these exercises.

<b>Data</b>

The dataset we will be using throughout these exercises is the
well-known `readmissions` dataset. You can access it or directly
download it through DataRobot’s public S3 bucket
[here](https://s3.amazonaws.com/datarobot_public_datasets/10k_diabetes.csv).
We have defined the variable `data` which can be used directly in
project creation and links to our training dataset.

``` r
data <- "https://s3.amazonaws.com/datarobot_public_datasets/10k_diabetes.csv"
```

### 1\. Connect to DataRobot \[3min\]

You should already know how to do that from the introductory script. If
you have a Yaml file it should be very straightforward\! Else, use the
`endpoint` and `token` variables to define your credentials.

``` r
# Possible solution
datarobot::ConnectToDataRobot(configPath = "../config.yaml")
```

    ## Authentication token saved

``` r
# After connecting, load the datarobot package
library(datarobot)
```

    ## Authenticating with config at: /Users/chester.ismay/.config/datarobot/drconfig.yaml

    ## Authentication token saved

### 2\. Create a Project \[10min\]

Create a DataRobot Project:

1.  Use the `data` variable as input.
2.  Set `readmitted` as the target.
3.  Start the project using explicitly the `quick` autopilot in the
    `mode` variable.
4.  Use `AUC` as the optimization metric.
5.  Set `worker_count` variable to -1 using the `UpdateProject()`
    function.
6.  Wait for Autopilot to complete.

While waiting, go through the documentation and the different settings
that exist.

**HINT**: To initiate the project with `quick` autopilot, you will have
to use two different functions: the `SetupProject()` function and the
`SetTarget()` function.

``` r
# Possible solution
project <- SetupProject(
  dataSource = data,
  projectName = "01_Feature_Selection_Curves - Exercise 1"
)
```

    ## Project 01_Feature_Selection_Curves - Exercise 1 creation requested, awaiting creation

    ## Project 5f3565ddc541da0ff3236e32 (01_Feature_Selection_Curves - Exercise 1) created

``` r
SetTarget(
  project = project,
  target = "readmitted",
  mode = "quick",
  metric = "AUC"
)
```

    ## Autopilot started

``` r
UpdateProject(project = project, workerCount = -1)
```

    ## Project 5f3565ddc541da0ff3236e32 updated

``` r
WaitForAutopilot(project)
```

    ## In progress: 13, queued: 0 (waited: 0s)

    ## In progress: 13, queued: 0 (waited: 3s)

    ## In progress: 13, queued: 0 (waited: 4s)

    ## In progress: 13, queued: 0 (waited: 6s)

    ## In progress: 13, queued: 0 (waited: 8s)

    ## In progress: 13, queued: 0 (waited: 11s)

    ## In progress: 13, queued: 0 (waited: 16s)

    ## In progress: 13, queued: 0 (waited: 23s)

    ## In progress: 13, queued: 0 (waited: 37s)

    ## In progress: 11, queued: 0 (waited: 58s)

    ## In progress: 7, queued: 0 (waited: 79s)

    ## In progress: 2, queued: 0 (waited: 100s)

    ## In progress: 7, queued: 0 (waited: 121s)

    ## In progress: 7, queued: 0 (waited: 141s)

    ## In progress: 7, queued: 0 (waited: 162s)

    ## In progress: 5, queued: 0 (waited: 183s)

    ## In progress: 3, queued: 0 (waited: 204s)

    ## In progress: 7, queued: 9 (waited: 225s)

    ## In progress: 7, queued: 8 (waited: 247s)

    ## In progress: 9, queued: 1 (waited: 268s)

    ## In progress: 6, queued: 0 (waited: 289s)

    ## In progress: 1, queued: 0 (waited: 310s)

    ## In progress: 0, queued: 0 (waited: 331s)

    ## In progress: 0, queued: 0 (waited: 352s)

    ## In progress: 0, queued: 0 (waited: 372s)

    ## In progress: 0, queued: 0 (waited: 394s)

    ## In progress: 0, queued: 0 (waited: 414s)

    ## In progress: 5, queued: 0 (waited: 436s)

    ## In progress: 5, queued: 0 (waited: 457s)

    ## In progress: 1, queued: 0 (waited: 478s)

    ## In progress: 1, queued: 0 (waited: 499s)

    ## In progress: 1, queued: 0 (waited: 520s)

    ## In progress: 1, queued: 0 (waited: 541s)

    ## In progress: 1, queued: 0 (waited: 561s)

    ## In progress: 1, queued: 0 (waited: 583s)

    ## In progress: 1, queued: 0 (waited: 604s)

    ## In progress: 1, queued: 0 (waited: 625s)

    ## In progress: 1, queued: 0 (waited: 646s)

    ## In progress: 1, queued: 0 (waited: 668s)

    ## In progress: 1, queued: 0 (waited: 689s)

    ## In progress: 1, queued: 0 (waited: 712s)

    ## In progress: 1, queued: 0 (waited: 734s)

    ## In progress: 1, queued: 0 (waited: 755s)

    ## In progress: 1, queued: 0 (waited: 775s)

    ## In progress: 1, queued: 0 (waited: 796s)

    ## In progress: 1, queued: 0 (waited: 818s)

    ## In progress: 1, queued: 0 (waited: 839s)

    ## In progress: 1, queued: 0 (waited: 859s)

    ## In progress: 0, queued: 0 (waited: 881s)

    ## In progress: 0, queued: 0 (waited: 903s)

    ## In progress: 0, queued: 0 (waited: 925s)

    ## In progress: 0, queued: 0 (waited: 946s)

    ## In progress: 0, queued: 0 (waited: 967s)

### 3\. Create Custom Feature Lists \[15min\]

Instructions:

1.  Retrieve Feature Impact for the most accurate model by the
    validation score.
2.  Create 3 different Feature Lists named `top_5`, `top_10` and
    `top_15`. Each featurelist will have the respective top \(n\)
    features based on Feature Impact score.

**HINT**: Search for the `CreateFeaturelist()` function. This will help
you create a new featurelist.

``` r
# Possible solution
best_model <- ListModels(project)[[1]]

fi_df <- GetFeatureImpact(best_model)

# Create vectors with correct features
top_5 <- head(fi_df, 5)$featureName
top_10 <- head(fi_df, 10)$featureName
top_15 <- head(fi_df, 15)$featureName
```

``` r
# Create Feature Lists
top_5_fl <- CreateFeaturelist(project, "top_5", featureNames = top_5)
```

    ## Featurelist top_5 created

``` r
top_10_fl <- CreateFeaturelist(project, "top_10", featureNames = top_10)
```

    ## Featurelist top_10 created

``` r
top_15_fl <- CreateFeaturelist(project, "top_15", featureNames = top_15)
```

    ## Featurelist top_15 created

### 4\. Identify Specific Models \[20min\]

Create a list with the models that fulfill the below requirements:

1.  Model’s `modelType` is ‘Light Gradient’, ‘eXtreme Gradient’, or
    ‘Elastic-Net’.
2.  Model’s `samplePct` equals 64% (The amount of data the model has
    been trained on).

**Hint**: Look into the `filter` variable of the `ListModels()`
function. That should help you find the models that have been trained on
64% of the data fast.

``` r
# Possible solution

# Define vector of model types we want
model_types <- c("eXtreme Gradient", "Elastic-Net", "Light Gradient")

# Get all models with 64% training sample.
models_64 <- ListModels(project, filter = list("samplePct" = 64))

# Filter models that belong to the specific model types.
final_models <- list()
i <- 1
for (model in models_64) {
  for (type in model_types) {
    if (grepl(type, model$modelType)) {
      final_models[[i]] <- model
      i <- i + 1
    }
  }
}
```

### 5\. Retrain models on the custom featurelists \[10 min\]

Use the list created in `step 4` to:

1.  Retrain the models with the top\_5, top\_10, top\_15 featurelists
    and 64% of the data.

<!-- end list -->

``` r
fl_list <- list(top_5_fl, top_10_fl, top_15_fl)
job_ids <- vector(mode = "character")
i <- 1
for (l in fl_list) {
  for (model in final_models) {
    job_ids[i] <- RequestNewModel(project,
      blueprint = list(
        blueprintId = model$blueprintId,
        project = project$projectId
      ),
      samplePct = 64,
      featurelist = list(featurelistId = l$featurelistId)
    )
    i <- i + 1
  }
}
```

    ## New model request received
    ## New model request received
    ## New model request received
    ## New model request received
    ## New model request received
    ## New model request received
    ## New model request received
    ## New model request received
    ## New model request received
    ## New model request received
    ## New model request received
    ## New model request received
    ## New model request received
    ## New model request received
    ## New model request received

``` r
# Wait for all jobs to complete
for (id in job_ids) {
  WaitForJobToComplete(project, id)
}
```

### 6\. Plot performance based on the different featurelists \[15min\]

Using the knowledge that you acquired from the previous questions:

1.  Create a list with all of the models retrained on `top_5`, `top_10`,
    and `top_15` feature lists.
2.  Find the average value of the cross-validation score based on AUC
    for the retrained models and plot that.

**Hint**: DataRobot Model objects have an element called
`featurelistName` which returns the name of the featurelist used to
train that model. That should help you find the models you are looking
for. Note that Blender models will often return `NA` as featurelist so
they are removed from the search below.

**Hint 2**: DataRobot will not calculate cross-validation scores
automatically for the retrained models. What you can do is call the
`CrossValidateModel()` function in order to calculate cross validation.

**Warning**: Do not forget to wait after you ask DataRobot to calculate
cross validation scores\!

``` r
# Proposed Solution

fl_names <- c("top_5", "top_10", "top_15")

# Get all models with 64% training sample.
models_64 <- ListModels(project, filter = list("samplePct" = 64))

nonblenders <- Filter(
  function(m) !grepl(pattern = "Blender", x = m$modelType),
  models_64
)
```

``` r
# Get all models that have been retrained
retrained_models <- list()
i <- 1
for (model in nonblenders) {
  for (fl in fl_names) {
    if (fl == model$featurelistName) {
      retrained_models[[i]] <- model
      i <- i + 1
    }
  }
}
```

``` r
# Calculate crossValidation scores for all of the retrained models
# if possible
job_ids <- vector(mode = "character")
i <- 1
for (model in ListModels(project)) {
  try(
    expr = {
      job_ids[i] <- CrossValidateModel(model = model)
      i <- i + 1
    },
    silent = TRUE
  )
}
```

    ## Cross validation request received
    ## Cross validation request received
    ## Cross validation request received
    ## Cross validation request received
    ## Cross validation request received
    ## Cross validation request received
    ## Cross validation request received
    ## Cross validation request received
    ## Cross validation request received
    ## Cross validation request received
    ## Cross validation request received
    ## Cross validation request received
    ## Cross validation request received
    ## Cross validation request received
    ## Cross validation request received
    ## Cross validation request received
    ## Cross validation request received
    ## Cross validation request received
    ## Cross validation request received
    ## Cross validation request received
    ## Cross validation request received
    ## Cross validation request received
    ## Cross validation request received
    ## Cross validation request received
    ## Cross validation request received
    ## Cross validation request received
    ## Cross validation request received
    ## Cross validation request received

``` r
# Wait for all the cross validation jobs to complete
for (id in job_ids) {
  WaitForJobToComplete(project, id)
}
```

``` r
# Query the retrained models again else they will not have their crossValidation score
# since they are based on the previous version of the project when `models_64` was created

# Get all non-blender models with 64% training sample.
models_64 <- ListModels(project, filter = list("samplePct" = 64))
nonblenders <- Filter(
  function(m) !grepl(pattern = "Blender", x = m$modelType),
  models_64
)

# Update retrained_models with new CV results
retrained_models <- list()
i <- 1
for (model in nonblenders) {
  for (fl in fl_names) {
    if (fl == model$featurelistName) {
      retrained_models[[i]] <- model
      i <- i + 1
    }
  }
}

# Empty vectors to hold results
top_5_cv_scores <- vector(mode = "numeric")
top_10_cv_scores <- vector(mode = "numeric")
top_15_cv_scores <- vector(mode = "numeric")

for (model in retrained_models) {
  if (model$featurelistName == "top_5") {
    top_5_cv_scores <- append(
      x = top_5_cv_scores,
      values = model$metrics$AUC$crossValidation
    )
  } else if (model$featurelistName == "top_10") {
    top_10_cv_scores <- append(
      x = top_10_cv_scores,
      values = model$metrics$AUC$crossValidation
    )
  } else {
    top_15_cv_scores <- append(
      x = top_15_cv_scores,
      values = model$metrics$AUC$crossValidation
    )
  }
}

values <- sapply(
  list(top_5_cv_scores, top_10_cv_scores, top_15_cv_scores),
  mean
)
names <- factor(c("top_5", "top_10", "top_15"),
  levels = c("top_5", "top_10", "top_15")
)
plot(names, values)
```

![](1.-R-API-Training---Feature-Selection-Curves-%5BSolution%5D_files/figure-gfm/unnamed-chunk-12-1.png)<!-- -->

What do you see?

### Verification

To verify that you have completed everything correctly, look at the
`Light Gradient Boosting on ElasticNet Predictions` model that was
trained on 64% of the data with the Informative Features featurelist.
The cross-validation score for AUC should be 0.6991.

### Bonus Question

You might have noticed that the first list of models we created had 5
(might change with DR releases) models. It makes sense that since we
want to run each one of these models using 3 different feature lists, we
should have ended with 15 models. That is not the case as we had 12
models in the end. Can you think why that is the case?

#### Bonus Question Answer:

The 5 models we retrieved are not unique. Some of them are the same just
trained on different featurelists. DataRobot identifies that we have
already initiated model building with a featurelist and it will not
create a duplicate model.
