6\. R API Training - Continuous Model Training \[Solution\]
================
Thodoris Petropoulos and Chester Ismay<br>Contributors: Rajiv Shah

This is the 6th exercise to complete in order to finish your `R API
Training for DataRobot` course\! This exercise teaches you how to deploy
a trained model, make predictions (**Warning**: Multiple ways of getting
predictions out of DataRobot), and monitor drift to replace a model.

Here are the actual sections of the notebook alongside time to complete:

1.  Connect to DataRobot. \[3min\]<br>
2.  Retrieve the first project created in `Exercise 4 - Model Factory`.
    \[5min\]
3.  Search for the `recommended for deployment` model and deploy it as a
    rest API. \[20min\]
4.  Create a scoring procedure using dataset (1) that will force data
    drift on that deployment. \[25min\]
5.  Check data drift. Does it look like data is drifting?. \[3min\]
6.  Create a new project using data (2). \[5min\]
7.  Replace the previously deployed model with the new `recommended for
    deployment` model from the new project. \[10min\]

Each section will have specific instructions so do not worry if things
are still blurry\!

As always, consult:

  - [API Documentation via CRAN
    Vignettes](https://CRAN.R-project.org/package=datarobot)
  - [Samples](https://github.com/datarobot-community/examples-for-data-scientists)
  - [Tutorials](https://github.com/datarobot-community/tutorials-for-data-scientists)

The last two links should provide you with the snippets you need to
complete most of these exercises.

<b>Data</b>

1)  The dataset we will be using throughout these exercises is the
    well-known `readmissions dataset`. You can access it or directly
    download it through DataRobot’s public S3 bucket
    [here](https://s3.amazonaws.com/datarobot_public_datasets/10k_diabetes.csv).

2)  This dataset will be used to retrain the model. It can be accessed
    [here](https://s3.amazonaws.com/datarobot_public_datasets/10k_diabetes_scoring.csv)
    through DataRobot’s public S3 bucket.

### Import Packages

Import packages here as you start finding out what packages are needed.
The DataRobot package is already included for your convenience.

``` r
# Proposed packages needed
library(dplyr)
```

    ## 
    ## Attaching package: 'dplyr'

    ## The following objects are masked from 'package:stats':
    ## 
    ##     filter, lag

    ## The following objects are masked from 'package:base':
    ## 
    ##     intersect, setdiff, setequal, union

``` r
library(purrr)
library(readr)
library(glue)
```

    ## 
    ## Attaching package: 'glue'

    ## The following object is masked from 'package:dplyr':
    ## 
    ##     collapse

``` r
library(lobstr)
```

### 1\. Connect to DataRobot. \[3min\]<br>

``` r
# Possible solution
datarobot::ConnectToDataRobot(configPath = "../config.yaml")
```

    ## Authentication token saved

``` r
# After connecting, load the datarobot package
library(datarobot)
```

### 2\. Retrieve the first project created in `Exercise 5 - Model Factory` . \[5min\]

This should be the first project created during the exercise. Not one of
the projects created using a sample of `admission_type_id`.

``` r
#Proposed Solution
project <- GetProject('YOUR_PROJECT_ID')
```

### 3\. Search for the `recommended for deployment` model and deploy it as a rest API. \[10min\]

**Hint**: The recommended model can be found using the
`GetModelRecommendation()` function.

**Hint 2**: Use the `UpdateDeploymentDriftTrackingSettings()` function
on the DataRobot Deployment object to enable data drift tracking.

``` r
# Proposed Solution

#Find the recommended model
recommended_model_info <- GetModelRecommendation(
  project,
  type = "Recommended for Deployment"
)
recommended_model <- GetModel(
  project,
  modelId = recommended_model_info$modelId
)
#Deploy the model
prediction_server <- ListPredictionServers()[[1]]

deployment <- CreateDeployment(
  model = recommended_model,
  label = "Readmissions Deployment",
  defaultPredictionServerId = prediction_server$id
)
UpdateDeploymentDriftTrackingSettings(
  deploymentId = deployment$id,
  featureDriftEnabled = TRUE
)
```

    ## $featureDrift
    ## $featureDrift$enabled
    ## [1] TRUE
    ## 
    ## 
    ## $associationId
    ## $associationId$columnNames
    ## NULL
    ## 
    ## $associationId$requiredInPredictionRequests
    ## [1] TRUE
    ## 
    ## 
    ## $targetDrift
    ## $targetDrift$enabled
    ## [1] FALSE
    ## 
    ## 
    ## $predictionIntervals
    ## $predictionIntervals$percentiles
    ## list()
    ## 
    ## $predictionIntervals$enabled
    ## [1] FALSE

### 4\. Create a scoring procedure using dataset (1) that will force data drift on that deployment. \[25min\]

**Instructions** 1. Take the first 100 rows of dataset (1) and save them
to a data frame 2. Score 5 times using these observations to force
drift. 3. Use the deployment you created during `question 3`.

**Hint**: The only thing you will have to change for the code to work is
change the filename variable to point to the csv file to be scored and
create a for loop.

``` r
# Proposed Solution
# Download first 100 rows of CSV file to be used
file_url <- "https://s3.amazonaws.com/datarobot_public_datasets/10k_diabetes.csv"
read_csv(file_url) %>% 
  head(100) %>% 
  write_csv(path = "scoring_dataset.csv")
```

    ## Parsed with column specification:
    ## cols(
    ##   .default = col_character(),
    ##   time_in_hospital = col_double(),
    ##   num_lab_procedures = col_double(),
    ##   num_procedures = col_double(),
    ##   num_medications = col_double(),
    ##   number_outpatient = col_double(),
    ##   number_emergency = col_double(),
    ##   number_inpatient = col_double(),
    ##   number_diagnoses = col_double(),
    ##   readmitted = col_logical()
    ## )

    ## See spec(...) for full column specifications.

``` r
# This code is a modified R version of the Python code available
# under the Deployments > Deployments > Readmissions Deployment >
# Prediction API > Single tab
# with Input Format chosen as CSV. Go there to get your 
# `DEPLOYMENT_ID`, `API_URL`, `API_KEY`, and 
# `DATAROBOT_KEY` to fill in below.

DEPLOYMENT_ID <- deployment$id
API_URL <- glue::glue("https://datarobot-predictions.orm.datarobot.com/predApi/v1.0/deployments/{DEPLOYMENT_ID}/predictions")
API_KEY <- " "
DATAROBOT_KEY <- " "

# Code to produce prediction
MAX_PREDICTION_FILE_SIZE_BYTES <- 52428800  # 50 MB

make_datarobot_deployment_predictions <- function(data, deployment_id) {
  # Set HTTP headers. The charset should match the contents of the file.
  headers_list <- list(
    'Content-Type' = 'text/plain; charset=UTF-8',
    'Authorization' = paste('Bearer', API_KEY),
    'DataRobot-Key' = DATAROBOT_KEY
  )
  headers <- purrr::as_vector(headers_list)
  # Make API request for predictions
  predictions_response <- httr::POST(
    url = API_URL,
    httr::add_headers(.headers = headers),
    body = httr::upload_file(filename)
  )

  raise_dataroboterror_for_status(predictions_response)
}

# Raise error if the request fails along with the 
# response returned
raise_dataroboterror_for_status <- function(response) {
  if (httr::http_error(response))
    stop(paste("Error:", httr::http_status(response)))
  else {
    message("Predictions with deployment completed.")
    # Return a JSON object in the format similar to
    # https://app.datarobot.com/docs/predictions/api/new-prediction-api.html#response-schema
    httr::content(response)
  }
}

# A usage demonstration of 
# `make_datarobot_deployment_predictions(data, deployment_id)`
main <- function(filename, deployment_id) {
  
  if (missing(filename))
    stop("Input file as `filename` is a required argument.")
  
  data <- readr::read_csv(filename, col_types = cols())
  data_size <- lobstr::obj_size(data)
  if (data_size >= MAX_PREDICTION_FILE_SIZE_BYTES) {
    stop(
      glue(
        "Input file is too large: {data_size} bytes. \\
        Max allowed size is: {MAX_PREDICTION_FILE_SIZE_BYTES} bytes."
      )
    )
  }
  return(make_datarobot_deployment_predictions(filename, deployment_id))
}
```

``` r
for (i in 1:5) {
  filename <- "scoring_dataset.csv"
  main(filename, deployment_id = DEPLOYMENT_ID)
}
```

    ## Predictions with deployment completed.
    ## Predictions with deployment completed.
    ## Predictions with deployment completed.
    ## Predictions with deployment completed.
    ## Predictions with deployment completed.

### 5\. Check data drift. Does it look like data is drifting?. \[3min\]

Check data drift from within the `Deployments` page in the UI for the
Readmissions Deployment. Is data drift marked as red?

### 6\. Create a new project using data (2). \[5min\]

Link to data:
<https://s3.amazonaws.com/datarobot_public_datasets/10k_diabetes_scoring.csv>

``` r
#Proposed Solution
new_project <- SetupProject(
  dataSource = "https://s3.amazonaws.com/datarobot_public_datasets/10k_diabetes_scoring.csv",
  projectName = "06_New_Project"
)
```

    ## Project 06_New_Project creation requested, awaiting creation

    ## Project 5f39d3dcc541da15fe236e4e (06_New_Project) created

``` r
SetTarget(
  project = new_project,
  target = "readmitted",
  mode = "quick"
)
```

    ## Autopilot started

``` r
UpdateProject(project = new_project, workerCount = -1)
```

    ## Project 5f39d3dcc541da15fe236e4e updated

``` r
WaitForAutopilot(project = new_project, verbosity = 0)
```

### 7\. Replace the previously deployed model with the new `recommended for deployment` model from the new project. \[10min\]

**Hint**: You will have to provide a reason why you are replacing the
model. Explore the `ModelReplacementReason` list for options.

``` r
#Proposed Solution
new_recommended_model_info <- GetModelRecommendation(
  project = new_project,
  type = "Recommended for Deployment"
)
new_recommended_model <- GetModel(
  project = new_project,
  modelId = new_recommended_model_info$modelId
)
ReplaceDeployedModel(
  deploymentId = deployment$id,
  newModelId = new_recommended_model$modelId,
  replacementReason = ModelReplacementReason$DataDrift
  )
```

    ## $model
    ## $projectId
    ## [1] "5f39d3dcc541da15fe236e4e"
    ## 
    ## $projectName
    ## [1] "06_New_Project"
    ## 
    ## $modelId
    ## [1] "5f39d6cbf93f5d47f179d8e7"
    ## 
    ## attr(,"class")
    ## [1] "dataRobotModel"
    ## 
    ## $defaultPredictionServer
    ## $url
    ## [1] "https://datarobot-predictions.orm.datarobot.com"
    ## 
    ## $id
    ## [1] "5b56f5308daae3002166d290"
    ## 
    ## $dataRobotKey
    ## [1] "cc0e0c01-8463-3e63-4794-5bea42900997"
    ## 
    ## attr(,"class")
    ## [1] "dataRobotPredictionServer"
    ## 
    ## $description
    ## [1] ""
    ## 
    ## $modelHealth
    ## $modelHealth$status
    ## [1] "unknown"
    ## 
    ## $modelHealth$startDate
    ## NULL
    ## 
    ## $modelHealth$message
    ## [1] "Either not enough predictions (100) have been made for data drift analysis, or predictions are still being processed. Please refresh the page in 1 minute if predictions have been made."
    ## 
    ## $modelHealth$endDate
    ## NULL
    ## 
    ## 
    ## $predictionUsage
    ## $predictionUsage$dailyRates
    ## [1] 0 0 0 0 0 0 0
    ## 
    ## $predictionUsage$lastTimestamp
    ## NULL
    ## 
    ## 
    ## $capabilities
    ## $capabilities$supportsModelReplacement
    ## [1] TRUE
    ## 
    ## $capabilities$supportsTargetDriftTracking
    ## [1] TRUE
    ## 
    ## $capabilities$supportsFeatureDriftTracking
    ## [1] TRUE
    ## 
    ## 
    ## $label
    ## [1] "Readmissions Deployment"
    ## 
    ## $serviceHealth
    ## $serviceHealth$status
    ## [1] "unknown"
    ## 
    ## $serviceHealth$startDate
    ## NULL
    ## 
    ## $serviceHealth$endDate
    ## NULL
    ## 
    ## 
    ## $accuracyHealth
    ## $accuracyHealth$status
    ## [1] "unavailable"
    ## 
    ## $accuracyHealth$startDate
    ## NULL
    ## 
    ## $accuracyHealth$message
    ## [1] "Accuracy is unknown."
    ## 
    ## $accuracyHealth$endDate
    ## NULL
    ## 
    ## 
    ## $id
    ## [1] "5f39d3989071ae22b6821e7d"
    ## 
    ## $permissions
    ##  [1] "CAN_SHARE_DEPLOYMENT_OWNERSHIP"   "CAN_MAKE_PREDICTIONS"            
    ##  [3] "CAN_DELETE_CHALLENGERS"           "CAN_UPDATE_DEPLOYMENT_THRESHOLDS"
    ##  [5] "CAN_EDIT_CHALLENGERS"             "CAN_VIEW"                        
    ##  [7] "CAN_SHARE"                        "CAN_SCORE_CHALLENGERS"           
    ##  [9] "CAN_REPLACE_MODEL"                "CAN_ADD_CHALLENGERS"             
    ## [11] "CAN_EDIT_DEPLOYMENT"              "CAN_SUBMIT_ACTUALS"              
    ## [13] "CAN_APPROVE_REPLACEMENT_MODEL"    "CAN_DELETE_DEPLOYMENT"           
    ## 
    ## attr(,"class")
    ## [1] "dataRobotDeployment"
