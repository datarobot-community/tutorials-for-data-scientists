---
title: "6. R API Training - Continuous Model Training [Solution]"
author: "Thodoris Petropoulos and Chester Ismay<br>Contributors: Rajiv Shah"
output: github_document
---

This is the 6th exercise to complete in order to finish your `R API Training for DataRobot` course! This exercise teaches you how to deploy a trained model, make predictions (**Warning**: Multiple ways of getting predictions out of DataRobot), and monitor drift to replace a model.

Here are the actual sections of the notebook alongside time to complete: 

1. Connect to DataRobot. [3min]<br>
2. Retrieve the first project created in `Exercise 4 - Model Factory`. [5min]
3. Search for the `recommended for deployment` model and deploy it as a rest API. [20min]
4. Create a scoring procedure  using dataset (1) that will force data drift on that deployment. [25min]
5. Check data drift. Does it look like data is drifting?. [3min]
6. Create a new project using data (2). [5min]
7. Replace the previously deployed model with the new `recommended for deployment` model from the new project. [10min]

Each section will have specific instructions so do not worry if things are still blurry!

As always, consult:

- [API Documentation via CRAN Vignettes](https://CRAN.R-project.org/package=datarobot)
- [Samples](https://github.com/datarobot-community/examples-for-data-scientists)
- [Tutorials](https://github.com/datarobot-community/tutorials-for-data-scientists)

The last two links should provide you with the snippets you need to complete most of these exercises.

<b>Data</b>

(1) The dataset we will be using throughout these exercises is the well-known `readmissions dataset`. You can access it or directly download it through DataRobot's public S3 bucket [here](https://s3.amazonaws.com/datarobot_public_datasets/10k_diabetes.csv).

(2) This dataset will be used to retrain the model. It can be accessed [here](https://s3.amazonaws.com/datarobot_public_datasets/10k_diabetes_scoring.csv) through DataRobot's public S3 bucket.

### Import Packages
Import packages here as you start finding out what packages are needed. The DataRobot package is already included for your convenience.

```{r}
# Proposed packages needed
library(dplyr)
library(purrr)
library(readr)
library(glue)
library(lobstr)
```


### 1. Connect to DataRobot. [3min]<br>

```{r}
# Possible solution
datarobot::ConnectToDataRobot(configPath = "../config.yaml")

# After connecting, load the datarobot package
library(datarobot)
```

### 2. Retrieve the first project created in `Exercise 5 - Model Factory` . [5min]

This should be the first project created during the exercise. Not one of the projects created using a sample of `admission_type_id`.

```{r eval=FALSE}
#Proposed Solution
project <- GetProject('YOUR_PROJECT_ID')
```


### 3. Search for the `recommended for deployment` model and deploy it as a rest API. [10min]

**Hint**: The recommended model can be found using the `GetModelRecommendation()` function. 

**Hint 2**: Use the `UpdateDeploymentDriftTrackingSettings()` function on the DataRobot Deployment object to enable data drift tracking.

```{r}
# Proposed Solution

#Find the recommended model
recommended_model_info <- GetModelRecommendation(
  project,
  type = "Recommended for Deployment"
)
recommended_model <- GetModel(
  project,
  modelId = recommended_model_info$modelId
)
#Deploy the model
prediction_server <- ListPredictionServers()[[1]]

deployment <- CreateDeployment(
  model = recommended_model,
  label = "Readmissions Deployment",
  defaultPredictionServerId = prediction_server$id
)
UpdateDeploymentDriftTrackingSettings(
  deploymentId = deployment$id,
  featureDriftEnabled = TRUE
)
```

### 4. Create a scoring procedure using dataset (1) that will force data drift on that deployment.  [25min]

**Instructions**
1. Take the first 100 rows of dataset (1) and save them to a data frame
2. Score 5 times using these observations to force drift.
3. Use the deployment you created during `question 3`.

**Hint**: The only thing you will have to change for the code to work is change the filename variable to point to the csv file to be scored and create a for loop.

```{r}
# Proposed Solution
# Download first 100 rows of CSV file to be used
file_url <- "https://s3.amazonaws.com/datarobot_public_datasets/10k_diabetes.csv"
read_csv(file_url) %>% 
  head(100) %>% 
  write_csv(path = "scoring_dataset.csv")
```


```{r}
# This code is a modified R version of the Python code available
# under the Deployments > Deployments > Readmissions Deployment >
# Prediction API > Single tab
# with Input Format chosen as CSV. Go there to get your 
# `DEPLOYMENT_ID`, `API_URL`, `API_KEY`, and 
# `DATAROBOT_KEY` to fill in below.

DEPLOYMENT_ID <- deployment$id
API_URL <- glue::glue("https://datarobot-predictions.orm.datarobot.com/predApi/v1.0/deployments/{DEPLOYMENT_ID}/predictions")
API_KEY <- " "
DATAROBOT_KEY <- " "

# Code to produce prediction
MAX_PREDICTION_FILE_SIZE_BYTES <- 52428800  # 50 MB

make_datarobot_deployment_predictions <- function(data, deployment_id) {
  # Set HTTP headers. The charset should match the contents of the file.
  headers_list <- list(
    'Content-Type' = 'text/plain; charset=UTF-8',
    'Authorization' = paste('Bearer', API_KEY),
    'DataRobot-Key' = DATAROBOT_KEY
  )
  headers <- purrr::as_vector(headers_list)
  # Make API request for predictions
  predictions_response <- httr::POST(
    url = API_URL,
    httr::add_headers(.headers = headers),
    body = httr::upload_file(filename)
  )

  raise_dataroboterror_for_status(predictions_response)
}

# Raise error if the request fails along with the 
# response returned
raise_dataroboterror_for_status <- function(response) {
  if (httr::http_error(response))
    stop(paste("Error:", httr::http_status(response)))
  else {
    message("Predictions with deployment completed.")
    # Return a JSON object in the format similar to
    # https://app.datarobot.com/docs/predictions/api/new-prediction-api.html#response-schema
    httr::content(response)
  }
}

# A usage demonstration of 
# `make_datarobot_deployment_predictions(data, deployment_id)`
main <- function(filename, deployment_id) {
  
  if (missing(filename))
    stop("Input file as `filename` is a required argument.")
  
  data <- readr::read_csv(filename, col_types = cols())
  data_size <- lobstr::obj_size(data)
  if (data_size >= MAX_PREDICTION_FILE_SIZE_BYTES) {
    stop(
      glue(
        "Input file is too large: {data_size} bytes. \\
        Max allowed size is: {MAX_PREDICTION_FILE_SIZE_BYTES} bytes."
      )
    )
  }
  return(make_datarobot_deployment_predictions(filename, deployment_id))
}
```

```{r}
for (i in 1:5) {
  filename <- "scoring_dataset.csv"
  main(filename, deployment_id = DEPLOYMENT_ID)
}
```


### 5. Check data drift. Does it look like data is drifting?. [3min]

Check data drift from within the `Deployments` page in the UI for the Readmissions Deployment. Is data drift marked as red?

### 6. Create a new project using data (2). [5min]

Link to data: <https://s3.amazonaws.com/datarobot_public_datasets/10k_diabetes_scoring.csv>

```{r}
#Proposed Solution
new_project <- SetupProject(
  dataSource = "https://s3.amazonaws.com/datarobot_public_datasets/10k_diabetes_scoring.csv",
  projectName = "06_New_Project"
)
SetTarget(
  project = new_project,
  target = "readmitted",
  mode = "quick"
)
UpdateProject(project = new_project, workerCount = -1)
WaitForAutopilot(project = new_project, verbosity = 0)
```

### 7. Replace the previously deployed model with the new `recommended for deployment` model from the new project. [10min]

**Hint**: You will have to provide a reason why you are replacing the model. Explore the `ModelReplacementReason` list for options.

```{r}
#Proposed Solution
new_recommended_model_info <- GetModelRecommendation(
  project = new_project,
  type = "Recommended for Deployment"
)
new_recommended_model <- GetModel(
  project = new_project,
  modelId = new_recommended_model_info$modelId
)
ReplaceDeployedModel(
  deploymentId = deployment$id,
  newModelId = new_recommended_model$modelId,
  replacementReason = ModelReplacementReason$DataDrift
  )
```

