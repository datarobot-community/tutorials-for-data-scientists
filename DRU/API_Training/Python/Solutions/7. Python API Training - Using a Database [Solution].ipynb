{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Python API Training - Using a Database [Solution]\n",
    "**Author**: Thodoris Petropoulos\n",
    "\n",
    "**Contributors**: Rajiv Shah\n",
    "\n",
    "This is the 7th exercise to complete in order to finish your Python API Training for DataRobot course! This exercise teaches you how to use a database in order to read datasets in order to train models or dump your predictions in a table.\n",
    "\n",
    "Here are the actual sections of the notebook alongside time to complete:\n",
    "\n",
    "1. Connect to DataRobot. [3min]\n",
    "2. Connect to the SQLite database provided. [5min]\n",
    "3. Load the `readmissions` dataset that needs scoring. [10min]\n",
    "4. Use one of the deployments generated earlier to score the dataset. [15min]\n",
    "5. Writeback predictions in the database\n",
    "\n",
    "As always, consult:\n",
    "\n",
    "[API Documentation](https://datarobot-public-api-client.readthedocs-hosted.com) <br>\n",
    "[Samples](https://github.com/datarobot-community/examples-for-data-scientists) <br>\n",
    "[Tutorials](https://github.com/datarobot-community/tutorials-for-data-scientists)\n",
    "\n",
    "The last two links should provide you with the snippets you need to complete most of these exercises.\n",
    "\n",
    "**Data**\n",
    "\n",
    "The dataset used in the current exercise can be reached via `databases/test_database.db`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import datarobot as dr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Connect to DataRobot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Possible solution\n",
    "dr.Client(config_path='../github/config.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Connect to the SQLite database provided. [5min]\n",
    "\n",
    "The SQLite database is under the folder `databases/test_database.db`. To create a connection, use the `sqlite3` library. It becomes apparent that while using Python, it really does not matter where your data is, you could just invoke the appropriate libraries and load your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Possible Solution\n",
    "conn = sqlite3.connect('databases/test_database.db')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Load the readmissions dataset that needs scoring. [10min]\n",
    "The readmissions dataset is saved within the `test_database`.\n",
    "\n",
    "**Instructions** \n",
    "1. Query the first 100 observations.\n",
    "2. Save them into a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Possible Solution\n",
    "df = pd.read_sql_query('SELECT * FROM readmissions LIMIT 100', conn)\n",
    "df.to_csv('dataset_to_be_scored.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Use one of the deployments generated earlier to score the dataset. [15min]\n",
    "**Instructions**\n",
    "1. Navigate to `Deployments` page within DataRobot.\n",
    "2. Find the Python code that allows you to make predictions using the API under `integrations` tab.\n",
    "3. Score the dataset and save the results in a new pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Possible Solution\n",
    "\n",
    "\"\"\"\n",
    "Usage:\n",
    "    python datarobot-predict.py <input-file.csv>\n",
    " \n",
    "This example uses the requests library which you can install with:\n",
    "    pip install requests\n",
    "We highly recommend that you update SSL certificates with:\n",
    "    pip install -U urllib3[secure] certifi\n",
    "\"\"\"\n",
    "import sys\n",
    "import json\n",
    "import requests\n",
    " \n",
    "API_KEY = ''\n",
    "DATAROBOT_KEY = ''\n",
    " \n",
    "DEPLOYMENT_ID = ''\n",
    " \n",
    "MAX_PREDICTION_FILE_SIZE_BYTES = 52428800  # 50 MB\n",
    " \n",
    " \n",
    "class DataRobotPredictionError(Exception):\n",
    "    \"\"\"Raised if there are issues getting predictions from DataRobot\"\"\"\n",
    " \n",
    " \n",
    "def make_datarobot_deployment_predictions(data, deployment_id):\n",
    "    \"\"\"\n",
    "    Make predictions on data provided using DataRobot deployment_id provided.\n",
    "    See docs for details:\n",
    "         https://app.eu.datarobot.com/docs/users-guide/predictions/api/new-prediction-api.html\n",
    " \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : str\n",
    "        Feature1,Feature2\n",
    "        numeric_value,string\n",
    "    deployment_id : str\n",
    "        The ID of the deployment to make predictions with.\n",
    " \n",
    "    Returns\n",
    "    -------\n",
    "    Response schema:\n",
    "        https://app.eu.datarobot.com/docs/users-guide/predictions/api/new-prediction-api.html#response-schema\n",
    " \n",
    "    Raises\n",
    "    ------\n",
    "    DataRobotPredictionError if there are issues getting predictions from DataRobot\n",
    "    \"\"\"\n",
    "    # Set HTTP headers. The charset should match the contents of the file.\n",
    "    headers = {\n",
    "        'Content-Type': 'text/plain; charset=UTF-8',\n",
    "        'Authorization': 'Bearer {}'.format(API_KEY),\n",
    "        'DataRobot-Key': DATAROBOT_KEY,\n",
    "    }\n",
    " \n",
    "    url = ''\\\n",
    "          'predictions'.format(deployment_id=deployment_id)\n",
    "    # Make API request for predictions\n",
    "    predictions_response = requests.post(\n",
    "        url,\n",
    "        data=data,\n",
    "        headers=headers,\n",
    "    )\n",
    "    _raise_dataroboterror_for_status(predictions_response)\n",
    "    # Return a Python dict following the schema in the documentation\n",
    "    return predictions_response.json()\n",
    " \n",
    " \n",
    "def _raise_dataroboterror_for_status(response):\n",
    "    \"\"\"Raise DataRobotPredictionError if the request fails along with the response returned\"\"\"\n",
    "    try:\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.HTTPError:\n",
    "        err_msg = '{code} Error: {msg}'.format(\n",
    "            code=response.status_code, msg=response.text)\n",
    "        raise DataRobotPredictionError(err_msg)\n",
    " \n",
    " \n",
    "def main(filename, deployment_id):\n",
    "    \"\"\"\n",
    "    Return an exit code on script completion or error. Codes > 0 are errors to the shell.\n",
    "    Also useful as a usage demonstration of\n",
    "    `make_datarobot_deployment_predictions(data, deployment_id)`\n",
    "    \"\"\"\n",
    "    if not filename:\n",
    "        print(\n",
    "            'Input file is required argument. '\n",
    "            'Usage: python datarobot-predict.py <input-file.csv>')\n",
    "        return 1\n",
    "    data = open(filename, 'rb').read()\n",
    "    data_size = sys.getsizeof(data)\n",
    "    if data_size >= MAX_PREDICTION_FILE_SIZE_BYTES:\n",
    "        print(\n",
    "            'Input file is too large: {} bytes. '\n",
    "            'Max allowed size is: {} bytes.'\n",
    "        ).format(data_size, MAX_PREDICTION_FILE_SIZE_BYTES)\n",
    "        return 1\n",
    "    try:\n",
    "        predictions = make_datarobot_deployment_predictions(data, deployment_id)\n",
    "    except DataRobotPredictionError as exc:\n",
    "        print(exc)\n",
    "        return 1\n",
    "    return predictions\n",
    "\n",
    "filename = 'dataset_to_be_scored.csv'\n",
    "result = main(filename, DEPLOYMENT_ID)\n",
    "result_df = pd.DataFrame(result['data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Writeback predictions in the database\n",
    "\n",
    "**Instructions**\n",
    "1. Join the results table with the original data used to score\n",
    "2. Save the results in a table called `prediction_results` within the `test_database_db`\n",
    "\n",
    "**Hint**: There are Pandas DataFrame methods that allow you to save the results to SQL and also append results if table already exists so keep that in mind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Possible Solution\n",
    "\n",
    "#They can be joined based on index without an issue\n",
    "final_scored_data = df.join(result_df)\n",
    "\n",
    "#Drop column with probabilities (Could also be preprocessed to get the actual predicted probability)\n",
    "final_scored_data.drop('predictionValues',axis=1,inplace=True)\n",
    "\n",
    "#Save results to the database\n",
    "final_scored_data.to_sql('prediction_results',conn,if_exists = 'append', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
