{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Python API Training - Feature Selection Curves [Exercise]\n",
    "\n",
    "<b>Author:</b> Thodoris Petropoulos <br>\n",
    "<b>Contributors:</b> Rajiv Shah\n",
    "\n",
    "Hey again! This is the first in a series of exercises to complete in order to finish your `Python API Training for DataRobot` course! This exercise will help you learn how to manipulate `DataRobot Models` and `Feature Lists`.\n",
    "\n",
    "Here are the actual sections of the notebook alongside time to complete: \n",
    "\n",
    "1. Connect to DataRobot. [3min]<br>\n",
    "2. Create a Project. [15min]<br>\n",
    "3. Create Custom Feature Lists. [15min]<br>\n",
    "4. Identify Specific Models. [20min]<br>\n",
    "5. Retrain Models on the custom feature lists. [10min] <br>\n",
    "6. Plot performance based on the different feature lists. [30min]\n",
    "7. Bonus Question\n",
    "\n",
    "Each section will have specific instructions so do not worry if things are still blurry!\n",
    "\n",
    "As always, consult:\n",
    "\n",
    "- [API Documentation](https://datarobot-public-api-client.readthedocs-hosted.com)\n",
    "- [Samples](https://github.com/datarobot-community/examples-for-data-scientists)\n",
    "- [Tutorials](https://github.com/datarobot-community/tutorials-for-data-scientists)\n",
    "\n",
    "The last two links should provide you with the snippets you need to complete most of these exercises.\n",
    "\n",
    "<b>Data</b>\n",
    "\n",
    "The dataset we will be using throughout these exercises is the well-known `readmissions dataset`. You can access it or directly download it through DataRobot's public S3 bucket [here](https://s3.amazonaws.com/datarobot_public_datasets/10k_diabetes.csv)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries\n",
    "Import libraries here as you start finding out what libraries are needed. The DataRobot package is already included for your convenience and we have also defined the variable `data` which can be used directly in project creation and links to our training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datarobot as dr\n",
    "\n",
    "data = 'https://s3.amazonaws.com/datarobot_public_datasets/10k_diabetes.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Connect to DataRobot [3min]\n",
    "\n",
    "You should already know how to do that from the introductory script. If you have a Yaml file it should be very straightforward! Else, use the `endpoint` and `token` variables to define your credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create a Project [10min]\n",
    "Create a DataRobot Project:\n",
    "\n",
    "1. Use the `data` variable as input.\n",
    "2. Set `readmitted` as the target.\n",
    "3. Start the project using explicitly the `quick` autopilot in the `mode` variable.\n",
    "4. Use `AUC` as the optimisation metric.\n",
    "5. Set `worker_count` variable to -1.\n",
    "6. Wait for Autopilot to complete. \n",
    "\n",
    "While waiting, go through the documentation and the different settings that exist.\n",
    "\n",
    "**HINT**: To initiate the project with `quick` autopilot, you will have to use two different methods. The `dr.Project.create` method and the `project.set_target` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create Custom Feature Lists [15min]\n",
    "\n",
    "Instructions:\n",
    "\n",
    "1. Retrieve Feature Impact for the most accurate model by the validation score\n",
    "2. Create 3 different Feature Lists named `top_5`, `top_10` and `top_15`. Each feature list will have the respective top n features based on Feature Impact score.\n",
    "\n",
    "**HINT**: Search for the `create_featurelist` method. This will help you create a new featurelist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Identify Specific Models [20min]\n",
    "\n",
    "Create a list with the models that fulfil the below requirements:\n",
    "\n",
    "1. Model's `model_type` is in ['Light Gradient', 'eXtreme Gradient', 'Elastic-Net'].\n",
    "2. Model's `sample_pct` equals 64% (The amount of data the model has been trained on).\n",
    "\n",
    "**Hint**: Look into the `search_params` variable of the `get_models` method. That should help you find the models that have been trained on 64% of the data fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Retrain models on the custom feature lists [10 min] \n",
    "\n",
    "Use the list created in `step 4` to: \n",
    "\n",
    "1. Retrain the models with the top_5, top_10, top_15 feature lists and 64% of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Plot performance based on the different feature lists [15min]\n",
    "\n",
    "Using the knowledge that you acquired from the previous questions:\n",
    "\n",
    "1. Create a list with all of the models retrained on top_5, top_10, and top 15 feature lists.\n",
    "2. Find the average value of the cross-validation score based on AUC for the retrained models and plot that.\n",
    "\n",
    "**Hint**: DataRobot Models have a method called `featurelist_name` which returns the name of the featurelist used to train that model. That should help you find the models you are looking for. Note that blender models will return `None` as feature list.\n",
    "\n",
    "**Hint 2**: DataRobot will not calculate cross validation scores automatically for the retrained models. What you can do is call the `cross_validate` method in order to calculate cross validation.\n",
    "\n",
    "**Warning**: Do not forget to wait after you ask DataRobot to calculate cross validation scores!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you see?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verification\n",
    "To verify that you have completed everything correctly, look at the `Light Gradient Boosting on ElasticNet Predictions` model that was trained on 64% of the data. The cross-validation score for AUC should be `0.6945`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus Question\n",
    "\n",
    "You might have noticed that the first list of models we created had 5 (might change with DR releases) models. It makes sense that since we want to run each one of these models using 3 different feature lists, we should have ended with 15 models. That is not the case as we had 12 models in the end. Can you think why that is the case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
